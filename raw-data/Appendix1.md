Okay, this is a fantastic list of compression ideas and a great starting point for populating an Appendix on Implementation Methods!

Let's expand each of these into a "mini-appendix" entry. I'll aim for a consistent structure for each:

*   **Name (Shorthand):** As provided.
*   **Core Idea:** A more detailed explanation of the method.
*   **LNA Implementation Strategy:**
    *   **Transform Type(s):** Which core or custom transform(s) would implement this.
    *   **Key Parameters (from Schema):** Examples of parameters needed.
    *   **Data Stored (HDF5 Paths):** Key datasets created by the transform.
    *   **Stash Interaction:** How it interacts with `DataHandle$stash`.
    *   **Invertibility Notes:** Specific considerations for the inverse step.
*   **Potential Benefits/Trade-offs:** Why it's useful, what challenges it might present.

---

## Appendix: Compendium of Compression & Encoding Methods for LNA

This appendix details various compression and encoding techniques that can be implemented within the LNA framework. Each method is described along with its core concept, a strategy for its integration into the LNA pipeline (as per v1.4 specification), and its potential benefits and trade-offs. These methods can often be chained together to achieve desired compression ratios and data characteristics.

---

### 1. Run-Shared Sub-Space (shared‐PCA/ICA)

*   **Core Idea:** Instead of fitting a separate spatial basis (e.g., Principal Components or Independent Components) for each individual fMRI run, a single, global spatial basis is learned from data pooled across all (or a subset of) runs for a subject/session. This exploits shared spatial patterns across runs, reducing redundancy. Each run is then projected onto this common basis, resulting in run-specific temporal coefficients but a single, shared basis matrix.

*   **LNA Implementation Strategy:**
    *   **Transform Type(s):**
        *   Primarily a `basis` transform.
        *   Could be a specific `method` within the standard `basis` transform (e.g., `method: "shared_pca"`) or a custom transform like `myorg.shared_basis`.
        *   Often used in conjunction with an `myorg.aggregate_runs` transform (see Method #10) to prepare the data for fitting the shared basis.
    *   **Key Parameters (from Schema):**
        *   `basis` transform: `method` (e.g., "pca", "ica"), `k` (number of components), `runs_for_basis_fit` (list of run IDs to use for fitting, if not all), `storage_order`.
        *   `myorg.aggregate_runs` (if used): `method` (e.g., "concatenate_time").
    *   **Data Stored (HDF5 Paths):**
        *   `/basis/global/matrix`: The single shared spatial basis matrix.
        *   `/basis/global/model_params`: (Optional) Mean, scaling factors if data was preprocessed before basis fitting.
        *   `/scans/{run_id}/embedding/coefficients`: Run-specific temporal coefficients obtained by projecting that run's data onto `/basis/global/matrix`. (This path would be generated by a subsequent `embed` transform that uses the shared basis).
    *   **Stash Interaction:**
        *   **Forward `myorg.aggregate_runs` (if used):** Consumes `initial_input_list`, produces `aggregated_matrix_for_basis`.
        *   **Forward `basis` (shared):** Consumes `aggregated_matrix_for_basis` (or iterates through multiple runs if aggregation isn't separate). Fits the basis. Writes basis to HDF5. *Outputs to stash*: path to the global basis (e.g., `global_basis_path`) for subsequent `embed` steps.
        *   **Forward `embed` (per run):** Consumes individual run data (e.g., `dense_mat_runX`) and `global_basis_path` from stash (or descriptor). Projects run data onto the global basis. Writes coefficients to HDF5. *Outputs to stash*: `coefficients_for_quant_runX`.
        *   **Inverse `embed` (per run):** Loads global basis and run-specific coefficients. Reconstructs run-specific data.
        *   **Inverse `basis` (shared):** May be a no-op if reconstruction is handled by `embed`. If it had model params like global mean, it might apply them.
        *   **Inverse `myorg.aggregate_runs` (if used):** If disaggregation is not supported, it passes the reconstructed aggregated data.
    *   **Invertibility Notes:** The inverse involves reconstructing each run's data using the shared basis and its specific coefficients. Reconstruction quality depends on how well the shared basis captures each run's variance.

*   **Potential Benefits/Trade-offs:**
    *   **Benefits:** Significant reduction in storage for basis matrices (one vs. N). Can lead to more stable basis components if individual runs are noisy or short.
    *   **Trade-offs:** May not capture run-specific spatial features as well as per-run bases. The choice of runs for fitting the global basis can influence its quality for other runs.

---

### 2. Octree Spatial PCA (“oct-PCA”)

*   **Core Idea:** This method applies Principal Component Analysis (PCA) locally within spatially distinct blocks (cubes) of the brain volume, organized in an octree (or k-d tree) structure. The volume is recursively subdivided into smaller cubes. PCA is performed within each leaf cube, capturing local spatial "texture" with a small number of components. A global, low-rank PCA might also be applied to the residuals or to the means of the blocks to capture large-scale patterns. The hierarchy helps maintain some continuity across block edges.

*   **LNA Implementation Strategy:**
    *   **Transform Type(s):**
        *   A specialized `basis` transform (e.g., `method: "octree_pca"`) or a custom transform `myorg.octree_pca`.
        *   It would internally manage the block decomposition and fitting.
    *   **Key Parameters (from Schema):**
        *   `max_depth` or `min_block_size`: To control octree subdivision.
        *   `k_local`: Number of PCA components per leaf block.
        *   `k_global` (optional): Number of components for a global residual PCA.
        *   `block_overlap` (optional): Pixels of overlap between blocks to reduce edge artifacts.
    *   **Data Stored (HDF5 Paths):**
        *   `/spatial/block_table`: A table defining the octree structure: block ID, parent ID, spatial coordinates (x0,x1,y0,y1,z0,z1 in masked voxel space), and possibly offsets into coefficient arrays.
        *   `/basis/blocks/{block_id}/matrix`: PCA basis matrix for each leaf block.
        *   `/basis/blocks/{block_id}/mean` (optional): Mean of data in that block if centered before PCA.
        *   `/basis/global_residual/matrix` (optional): Basis for global residual PCA.
        *   `/scans/{run_id}/embedding/coefficients_block_{block_id}`: Coefficients for each block for each run. (Alternatively, all block coefficients for a run could be concatenated into a single dataset, with `block_table` providing indexing info).
        *   `/scans/{run_id}/embedding/coefficients_global_residual` (optional).
    *   **Stash Interaction:**
        *   **Forward:** Consumes `dense_mat`. Internally segments data by blocks defined in `block_table`. Fits PCA per block, stores bases. Projects data per block, stores coefficients. *Outputs to stash*: paths to all coefficient sets and the `block_table` path (or signals their presence for subsequent quantization steps, if coefficients are quantized together).
        *   **Inverse:** Loads `block_table`. For each block, loads its basis and corresponding coefficients. Reconstructs data within each block. Stitches reconstructed blocks back together. Adds back global residual reconstruction if used. *Outputs to stash*: `dense_mat_reconstructed`.
    *   **Invertibility Notes:** Reconstruction involves applying the inverse PCA within each block and then assembling the blocks. Overlap handling during reconstruction is important if used during the forward pass.

*   **Potential Benefits/Trade-offs:**
    *   **Benefits:** Adapts to local spatial complexity. Can achieve good compression by using few components in spatially smooth regions. Potentially better at capturing fine-grained local details than a single global PCA.
    *   **Trade-offs:** Increased complexity in implementation. Potential for block artifacts if not handled carefully (e.g., with overlap). `block_table` adds metadata overhead. Managing coefficients for many small blocks can be intricate.

---

### 3. Hierarchical Bit-Plane Packing + Zstandard (zstd)

*   **Core Idea:** After data has been quantized to a low bit-depth (e.g., 4-8 bit integers), this technique reorganizes the data before general-purpose compression. Instead of storing voxel time series contiguously, it extracts bit-planes (e.g., all most significant bits (MSBs) from a local neighborhood of voxels, then all second MSBs, etc.). These bit-planes, especially MSBs, tend to have low entropy (long runs of 0s or 1s) due to spatial smoothness in the original signal. These reorganized bit-streams are then highly compressible by an entropy coder like Zstandard (zstd).

*   **LNA Implementation Strategy:**
    *   **Transform Type(s):**
        *   A custom transform, e.g., `myorg.bitplane_pack`.
        *   This transform would typically follow a `quant` transform and possibly a `delta` transform.
        *   The actual Zstandard compression would ideally be an HDF5 filter, transparently applied when `myorg.bitplane_pack` writes its output dataset. If Zstd isn't available as a direct HDF5 filter, the transform would apply Zstd itself and store the compressed byte stream.
    *   **Key Parameters (from Schema):**
        *   `bit_depth_input`: Expected bit depth from the previous `quant` step.
        *   `packing_order`: Strategy for interleaving (e.g., "voxel_major_bit_plane", "bit_plane_major_voxel").
        *   `neighborhood_size` (optional): Spatial extent for grouping bits before packing.
    *   **Data Stored (HDF5 Paths):**
        *   `/scans/{run_id}/packed_streams/data_stream`: The single dataset containing the bit-plane packed and Zstd-compressed data.
        *   Original dimensions and bit depth would be stored in this transform's descriptor to allow unpacking.
    *   **Stash Interaction:**
        *   **Forward:** Consumes quantized (and possibly delta-coded) integer data (e.g., `quantized_coefficients`) from stash. Performs bit-plane packing. Applies Zstd. Writes the compressed stream. *Outputs to stash*: Reference to the packed stream path.
        *   **Inverse:** Loads the compressed stream. Applies Zstd decompression. Unpacks bit-planes to reconstruct the integer data. *Outputs to stash*: `quantized_coefficients_reconstructed` (or whatever key the preceding inverse step expects).
    *   **Invertibility Notes:** Lossless if the preceding quantization is considered the source. The packing/unpacking and Zstd are lossless operations.

*   **Potential Benefits/Trade-offs:**
    *   **Benefits:** Can achieve very high compression ratios on quantized, smooth data, as Zstd is excellent for data with low entropy and repetitive patterns.
    *   **Trade-offs:** Adds computational overhead for packing/unpacking. Effectiveness heavily depends on the characteristics of the quantized data (smoothness, bit depth). Requires Zstd availability (as HDF5 filter or library).

---

### 4. Auto-Encoder Residual Codec (tiny 1 × 1 conv-AE)

*   **Core Idea:** After an initial dimensionality reduction (e.g., via PCA from a `basis` and `embed` transform, resulting in coefficient tensors), a small convolutional autoencoder (AE) is applied. This AE might use 1x1 convolutions (effectively per-coefficient-channel fully connected layers) or small 3D convolutions if there's spatial structure in the coefficient tensor. The AE attempts to learn a more compact representation. Instead of storing the AE's direct output, the *residual* between the AE's reconstruction and its input (the PCA coefficients) is quantized to a very low bit depth (e.g., 2-3 bits) and stored. The AE weights themselves are also stored.

*   **LNA Implementation Strategy:**
    *   **Transform Type(s):**
        *   A custom transform, e.g., `myorg.ae_residual_codec`.
        *   This would typically follow an `embed` transform (or any transform that produces a coefficient tensor).
    *   **Key Parameters (from Schema):**
        *   `ae_architecture`: Definition of the AE layers (e.g., number of layers, filter sizes, activation functions).
        *   `training_params`: (If AE is trained online) Epochs, learning rate, etc. Or `pretrained_weights_path`.
        *   `residual_bits`: Target bit depth for quantizing the residual.
    *   **Data Stored (HDF5 Paths):**
        *   `/plugins/myorg_ae_residual_codec/transform_XX/ae_weights`: The trained weights of the autoencoder.
        *   `/scans/{run_id}/ae_residuals/transform_YY/residual_codes`: The low-bit quantized residuals.
        *   (Optionally) The AE's direct latent representation if it's more compact and useful than reconstructing from residuals for some applications, though the "residual codec" idea implies storing residuals.
    *   **Stash Interaction:**
        *   **Forward:** Consumes coefficient tensor (e.g., `coefficients_from_embed`) from stash. Trains or loads AE. Passes coefficients through AE to get reconstruction `coeffs_ae_reconstructed`. Computes `residual = coefficients_from_embed - coeffs_ae_reconstructed`. Quantizes `residual` to `residual_codes`. Writes `ae_weights` and `residual_codes`. *Outputs to stash*: Path to `residual_codes` (and `ae_weights` if needed by next inverse step, though typically just `residual_codes` are passed on if the next step is generic quantization of these codes).
        *   **Inverse:** Loads `ae_weights` and `residual_codes`. De-quantizes `residual_codes` to get `residual_hat`. Passes a zero tensor (or a guess) through the AE decoder part (if AE is non-trivial) or reconstructs the AE's primary output. Then, `coefficients_reconstructed = ae_output_reconstructed + residual_hat`. (The exact reconstruction depends on what the AE is designed to predict vs. what the residual represents). A simpler AE might just have `coefficients_reconstructed = ae_decoder(latent_representation_derived_from_input) + dequantized_residual`. *Outputs to stash*: `coefficients_reconstructed_from_ae`.
    *   **Invertibility Notes:** Lossy due to residual quantization. The AE itself might be slightly lossy. Reconstruction quality depends on AE capacity and residual bit depth.

*   **Potential Benefits/Trade-offs:**
    *   **Benefits:** Can capture non-linear structures missed by PCA. Potentially very high compression if residuals are small and highly quantizable.
    *   **Trade-offs:** Much more complex to implement and train. AE training can be computationally expensive. Storing AE weights adds overhead (though usually small if AE is tiny). Sensitive to hyperparameters.

---

### 5. Δ-Frame / Run-Length Temporal Coding (delta)

*   **Core Idea:** Exploits temporal smoothness in signals (like fMRI time series or coefficients derived from them). The first frame (or value in a 1D series) is stored verbatim (or with minimal quantization). Subsequent frames are stored as the signed difference (delta) from the previous frame. These deltas are often smaller in magnitude and have a more peaked distribution than the original values, making them more compressible. Run-length encoding (RLE) can then be applied to these deltas (especially after quantization) to compress sequences of identical delta values (particularly zeros). Further entropy coding (e.g., Arithmetic or Range coding) can be applied to the RLE symbols or directly to the deltas.

*   **LNA Implementation Strategy:**
    *   **Transform Type(s):**
        *   The core `delta` transform.
    *   **Key Parameters (from Schema for `delta`):**
        *   `order`: Typically `1` for frame-to-frame differences.
        *   `axis`: The axis along which to compute deltas (e.g., time axis).
        *   `reference_value_storage`: `"first_value_verbatim"` (stores first frame/value separately), `"reconstruct_from_deltas"` (implies first value is zero or known).
        *   `delta_quantization_bits` (optional, if deltas are quantized within this step, otherwise a subsequent `quant` step is used): Bit depth for deltas.
        *   `coding_method` (optional): `"none"`, `"rle"`, `"arithmetic"`, `"range_coded"`. If not part of `delta`, these could be subsequent transforms or part of a `quant` step on deltas. (The spec implies `delta` itself might handle range coding).
    *   **Data Stored (HDF5 Paths):**
        *   `/scans/{run_id}/deltas/transform_XX/first_values`: The first frame/value if `reference_value_storage="first_value_verbatim"`.
        *   `/scans/{run_id}/deltas/transform_XX/delta_stream`: The (potentially RLE/entropy coded) delta values.
        *   Metadata in descriptor: `order`, `axis`, `coding_method`.
    *   **Stash Interaction:**
        *   **Forward:** Consumes data (e.g., `coefficients_for_delta` or `quantized_data_for_delta`) from stash. Computes deltas. Optionally RLE/entropy codes them. Writes first values and delta stream. *Outputs to stash*: Reference to delta stream path (and first values path if needed by next step, though usually these are consumed by the `delta` inverse itself).
        *   **Inverse:** Loads first values and delta stream. Decompresses/decodes deltas (inverse RLE/entropy coding). Reconstructs the original series by cumulatively adding deltas to the first value. *Outputs to stash*: `reconstructed_from_delta`.
    *   **Invertibility Notes:** Lossless if no quantization is applied to deltas within this step. If deltas are quantized, it becomes lossy.

*   **Potential Benefits/Trade-offs:**
    *   **Benefits:** Very effective for temporally smooth data. Simple concept. Can significantly reduce data range and improve statistics for subsequent entropy coding.
    *   **Trade-offs:** Error propagation: an error in a delta value affects all subsequent reconstructed values. Less effective if data has frequent large jumps.

---

### 6. Per-Voxel μ-Law + Entropy Coding (quant)

*   **Core Idea:** Instead of uniform quantization, which allocates quantization levels evenly across the data range, μ-law companding (or A-law) provides a non-uniform quantization. It allocates more quantization levels to smaller-magnitude values and fewer to larger-magnitude values, effectively providing higher resolution for low-amplitude signals where human perception (e.g., for audio) is more sensitive. For fMRI, this might preserve subtle BOLD fluctuations better. Lloyd-Max quantization is an optimal scalar quantizer that minimizes mean squared error for a given number of levels and a known data distribution. After non-uniform quantization, the resulting integer symbols are entropy coded (e.g., Huffman, Arithmetic, Range coding). This can be applied per-voxel, adapting the μ-law/Lloyd-Max parameters to each voxel's specific data distribution and range.

*   **LNA Implementation Strategy:**
    *   **Transform Type(s):**
        *   The core `quant` transform.
    *   **Key Parameters (from Schema for `quant`):**
        *   `method`: `"uniform"`, `"mu_law"`, `"a_law"`, `"lloyd_max"`.
        *   `bits`: Desired bit depth for quantized symbols.
        *   `mu_value` (if `method="mu_law"`): The μ parameter.
        *   `scope`: `"voxel"`, `"global"`. If "voxel", parameters (like range for μ-law, or codebook for Lloyd-Max) are computed and stored per voxel.
        *   `entropy_coder` (optional): `"none"`, `"huffman"`, `"arithmetic"`.
    *   **Data Stored (HDF5 Paths):**
        *   `/scans/{run_id}/quant_params/transform_XX/min_max_or_codebook`: If `scope="voxel"`, per-voxel min/max (for μ-law scaling) or Lloyd-Max codebooks. If `scope="global"`, single global min/max or codebook.
        *   `/scans/{run_id}/quant_symbols/transform_XX/symbols`: The quantized integer symbols (potentially entropy coded).
    *   **Stash Interaction:**
        *   **Forward:** Consumes data (e.g., `coefficients_to_quantize`) from stash. Computes quantization parameters based on `scope` and `method`. Applies quantization. Optionally entropy codes. Writes params and symbols. *Outputs to stash*: Reference to symbols path (and params if needed by next step, though usually consumed by `quant` inverse).
        *   **Inverse:** Loads quantization params and symbols. Decodes symbols (if entropy coded). Applies inverse quantization (e.g., inverse μ-law expansion, Lloyd-Max lookup). *Outputs to stash*: `reconstructed_from_quant`.
    *   **Invertibility Notes:** Inherently lossy due to quantization. The choice of method and bit depth determines the information loss.

*   **Potential Benefits/Trade-offs:**
    *   **Benefits:** μ-law/Lloyd-Max can provide better perceptual quality or lower MSE for a given bit rate compared to uniform quantization, especially if data has a non-uniform distribution. Per-voxel adaptation is powerful.
    *   **Trade-offs:** More complex than uniform quantization. Storing per-voxel parameters (if `scope="voxel"`) adds metadata overhead, which might negate some compression gains if bit depth is very low or there are few time points. Lloyd-Max requires iterative training per voxel if distributions are unknown.

---

### 7. Motion / Nuisance Projection Before Compression

*   **Core Idea:** Before applying primary compression techniques like PCA, regress out known sources of noise or "nuisance" variance from the fMRI data. Common nuisance regressors include head motion parameters (and their derivatives/quadratics), mean signals from white matter (WM) and cerebrospinal fluid (CSF) (often considered non-neural), and low-frequency scanner drifts (e.g., via polynomial basis functions or DCT). Removing this variance can "clean" the data, potentially allowing subsequent PCA/ICA to capture true neural signals with fewer components, or improving the compressibility of residuals.

*   **LNA Implementation Strategy:**
    *   **Transform Type(s):**
        *   A custom transform, e.g., `myorg.nuisance_projection` or `myorg.denoise`.
        *   This transform would typically be one of the *first* in the pipeline, operating on the `dense_mat` (voxel x time data) before any `basis` or `embed` steps.
    *   **Key Parameters (from Schema):**
        *   `confound_paths`: List of HDF5 paths (or references to `handle$meta`) to datasets containing confound regressors (e.g., motion params, WM/CSF signals).
        *   `regression_type`: `"linear_ols"`.
        *   `include_derivatives`, `include_ quadratics` (for motion params).
        *   `detrend_method`: `"polynomial"` (with `poly_degree`), `"dct_basis"` (with `dct_cutoff_secs`).
    *   **Data Stored (HDF5 Paths):**
        *   This transform might not store large numerical payloads itself if its purpose is solely to clean data for subsequent steps. It *could* store the beta coefficients of the nuisance regression if that's desired for analysis, e.g., `/analysis/nuisance_betas/transform_XX/betas`.
        *   Its descriptor would list the confound paths and parameters used.
    *   **Stash Interaction:**
        *   **Forward:** Consumes `dense_mat` from stash. Loads confound regressors (from file or `handle$meta`). Performs regression, obtaining cleaned data `dense_mat_cleaned = dense_mat - X_confounds * betas`. *Outputs to stash*: `dense_mat_cleaned` (under a key like `denoised_dense_mat` for the next transform, e.g., `basis`, to consume).
        *   **Inverse:** This is tricky. True inversion (adding back the exact nuisance variance) is usually not desired for analysis. The "inverse" might:
            1.  Be a no-op, passing through the reconstructed (but still denoised) data from later inverse steps.
            2.  If beta coefficients were stored, and if original confounds are available/stashed, it *could* add back `X_confounds * betas_hat`. This is rarely done.
            3.  Most likely, it's a conceptual pass-through; the goal of reading is the denoised data.
    *   **Invertibility Notes:** Generally considered a "lossy" preprocessing step from the perspective of raw signal fidelity, as variance is intentionally removed. The "inversion" is more about reconstructing the *cleaned* signal after subsequent compression steps have been inverted.

*   **Potential Benefits/Trade-offs:**
    *   **Benefits:** Can improve SNR of the data passed to compression. May lead to more meaningful components from PCA/ICA. Can reduce overall data variance, potentially improving compressibility.
    *   **Trade-offs:** Risk of removing neural signal if nuisance regressors are correlated with neural activity (e.g., motion correlated with task). Adds a processing step. Defining the "inverse" requires care.

---

### 8. Temporal Basis Swaps (temporal) – DCT ↔︎ Cubic B-Splines ↔︎ DPSS

*   **Core Idea:** Instead of representing temporal information purely as a sequence of values (or coefficients per time point), the time dimension can be projected onto a temporal basis set. The Discrete Cosine Transform (DCT) is common for this (capturing frequency content). Cubic B-splines offer local temporal support and smoothness, good for modeling event-related responses or slow drifts. Discrete Prolate Spheroidal Sequences (DPSS), or Slepian sequences, are time-frequency optimal and can be good for signals with specific band-pass characteristics or for robust detrending. The choice of basis can impact how well temporal features are captured and compressed.

*   **LNA Implementation Strategy:**
    *   **Transform Type(s):**
        *   The core `temporal` transform.
    *   **Key Parameters (from Schema for `temporal`):**
        *   `kind`: `"dct"`, `"bspline"`, `"dpss"`, `"polynomial"`.
        *   `n_basis` or `order`: Number of temporal basis functions.
        *   For `"bspline"`: `knot_vector_path` (path to HDF5 dataset with knot locations) or `knot_spacing_method`.
        *   For `"dpss"`: `time_bandwidth_product` (NW), `n_tapers`.
        *   `scope`: `"global"` (one temporal basis for all voxels/spatial components) or `"voxelwise"` (separate temporal fit per voxel/component, less common for pure compression).
    *   **Data Stored (HDF5 Paths):**
        *   `/temporal_basis/transform_XX/matrix`: The temporal basis matrix itself (e.g., Time x n_basis).
        *   `/temporal_basis/transform_XX/knots` (if `kind="bspline"` and knots are explicitly stored).
        *   `/scans/{run_id}/temporal_coeffs/transform_YY/coefficients`: The coefficients resulting from projecting data onto the temporal basis (e.g., Voxel x n_basis).
    *   **Stash Interaction:**
        *   **Forward:** Consumes data (e.g., `dense_mat` or `spatial_coefficients`) from stash. Generates or loads temporal basis matrix. Projects input data onto this basis. Writes temporal basis matrix (if not already standard and implied by params) and the resulting coefficients. *Outputs to stash*: `temporal_coefficients` for the next step.
        *   **Inverse:** Loads temporal basis matrix and temporal coefficients. Reconstructs the data by multiplying coefficients with the basis: `data_reconstructed = temporal_coefficients %*% t(temporal_basis_matrix)`. *Outputs to stash*: `reconstructed_from_temporal_basis`.
    *   **Invertibility Notes:** Lossy if `n_basis` is less than the original number of time points. Reconstruction quality depends on how well the chosen basis and number of functions capture the temporal dynamics.

*   **Potential Benefits/Trade-offs:**
    *   **Benefits:** Can significantly reduce temporal dimensionality. Different bases are suited for different signal characteristics (DCT for general frequencies, B-splines for localized events, DPSS for band-limited signals).
    *   **Trade-offs:** Choice of basis and number of components is crucial and data-dependent. A poor choice can lead to significant loss of temporal information or introduction of artifacts.

---

### 9. Sparse PCA / Sparse Dictionary Learning (L₁-penalty)

*   **Core Idea:** Standard PCA produces basis vectors (spatial components) that are generally dense, meaning most voxels have non-zero loadings. Sparse PCA (or sparse dictionary learning) adds an L₁ penalty (or other sparsity-inducing penalty) during the component estimation process. This forces many of the loadings in each basis vector to be exactly zero. The result is a set of spatially sparse components, which can be more interpretable (e.g., representing distinct functional networks) and more compressible, as only the non-zero loadings and their locations need to be stored.

*   **LNA Implementation Strategy:**
    *   **Transform Type(s):**
        *   A custom transform, e.g., `myorg.sparse_pca` or `myorg.sparse_dictionary`.
        *   This would be a type of `basis` transform.
    *   **Key Parameters (from Schema):**
        *   `k`: Number of sparse components.
        *   `alpha` or `lambda`: Sparsity penalty parameter(s).
        *   `method`: Specific sparse PCA algorithm (e.g., "spca_package_XYZ", "online_dict_learn").
        *   `storage_format`: `"csc"` (Compressed Sparse Column), `"csr"`, `"coordinate_list"` for the sparse basis matrix.
    *   **Data Stored (HDF5 Paths):**
        *   `/basis/global_sparse/transform_XX/data`: Non-zero values of the sparse basis matrix.
        *   `/basis/global_sparse/transform_XX/indices`: Row indices for CSC/CSR.
        *   `/basis/global_sparse/transform_XX/indptr`: Column pointers for CSC/CSR.
        *   `/basis/global_sparse/transform_XX/shape`: Dimensions of the original dense matrix.
        *   (And potentially mean/scale vectors if data was preprocessed).
    *   **Stash Interaction:**
        *   **Forward:** Consumes `dense_mat` (or `aggregated_matrix`). Fits sparse PCA model. Stores the sparse basis matrix in a sparse format. *Outputs to stash*: Path to the sparse basis components (or a handle to it) for a subsequent `embed` step.
        *   **Inverse:** Loads the sparse basis matrix (reconstructing it into a sparse matrix object in R, like `Matrix::dgCMatrix`). If followed by an `embed` step, `invert_step.embed` would provide coefficients, and this `invert_step.sparse_pca` would use them to reconstruct `dense_mat_reconstructed = coefficients %*% t(sparse_basis_matrix)`.
    *   **Invertibility Notes:** Lossy, as it's a dimensionality reduction technique. The degree of loss depends on `k` and how well the sparse components capture the data variance. Reconstruction involves sparse matrix operations.

*   **Potential Benefits/Trade-offs:**
    *   **Benefits:** Improved interpretability of components. Significant storage savings for the basis matrix if high sparsity is achieved. Can be more robust to noise than standard PCA.
    *   **Trade-offs:** Computationally more expensive than standard PCA. Sparsity level needs careful tuning. Choice of sparse PCA algorithm can impact results. Requires libraries for sparse matrix operations and fitting sparse models.

---

### 10. Cross-Run Aggregation + Encoder

*   **Core Idea:** This is a meta-strategy that combines data from multiple fMRI runs *before* applying a primary encoding or dimensionality reduction transform (like PCA, Sparse PCA, Octree PCA, etc.). Aggregation methods can include:
    1.  **Concatenating time series:** Stacking the time series from all runs for each voxel, creating a very long "mega-run" before fitting a global encoder.
    2.  **Averaging covariance matrices:** Computing a spatial covariance matrix for each run, averaging these covariance matrices, and then performing an eigen-decomposition on the average covariance to get a global basis.
    The resulting global basis (or other encoding model parameters) is then used to process each individual run.

*   **LNA Implementation Strategy:**
    *   **Transform Type(s):** This is a sequence of (at least) two transforms:
        1.  An aggregation transform: e.g., `myorg.aggregate_runs`.
        2.  An encoding transform: e.g., `basis` (with `method="pca"`), `myorg.sparse_pca`, `myorg.octree_pca`.
        (Followed by `embed` to apply the global model to individual runs, and then potentially `quant`, `delta`, etc.)
    *   **Key Parameters (from Schema):**
        *   `myorg.aggregate_runs`: `aggregation_method` ("concatenate_time", "average_covariance"), `runs_included` (list of run IDs that were aggregated, recorded by writer).
        *   Parameters for the subsequent encoder (e.g., for `myorg.sparse_pca`: `k`, `alpha`).
    *   **Data Stored (HDF5 Paths):**
        *   The `myorg.aggregate_runs` descriptor stores its parameters. It typically doesn't write large numerical payloads itself.
        *   The subsequent encoder (e.g., `myorg.sparse_pca`) stores its global model (e.g., `/basis/global_aggregated_spca/matrix`).
        *   An `embed` step then stores per-run coefficients using this global model (e.g., `/scans/{run_id}/embedding_from_aggregated/coefficients`).
    *   **Stash Interaction:** (As detailed in the previous recipe card response)
        *   **Forward `myorg.aggregate_runs`:** Consumes `initial_input_list` (list of run data arrays). Produces `aggregated_data_for_encoder` in stash.
        *   **Forward `encoder` (e.g., `myorg.sparse_pca`):** Consumes `aggregated_data_for_encoder`. Fits the global model. Writes model to HDF5. *Outputs to stash*: Path to the global model (e.g., `global_model_path`).
        *   **Forward `embed` (per run, if global model applied back to individual runs):** Consumes individual run data (`dense_mat_runX`) and `global_model_path`. Projects run data using the global model. Writes coefficients. *Outputs to stash*: `coefficients_for_quant_runX`.
        *   **Inverse `embed`:** Reconstructs individual run data using global model and run-specific coefficients.
        *   **Inverse `encoder`:** May be a no-op if `embed` handles reconstruction using the model.
        *   **Inverse `myorg.aggregate_runs`:** Typically passes through the reconstructed aggregated data unless complex disaggregation logic is implemented (rare).
    *   **Invertibility Notes:** Reconstruction yields either the individual runs (if an `embed` step applies the global model back to them) or the reconstructed aggregated data. The quality depends on the chosen encoder and how well a single global model fits all runs.

*   **Potential Benefits/Trade-offs:**
    *   **Benefits:** Can learn more robust and stable global patterns by leveraging data from all runs. Reduces model storage significantly compared to per-run models.
    *   **Trade-offs:** May obscure run-specific features if the global model is too coarse. The aggregation method itself can influence results (e.g., time concatenation assumes stationary statistics across runs to some extent). Increased complexity in the transform chain.

---

This detailed appendix should provide a solid reference for how these diverse compression ideas map onto the LNA v1.4 specification and its core components.