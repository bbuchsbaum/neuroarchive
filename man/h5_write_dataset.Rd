% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/utils_hdf5.R
\name{h5_write_dataset}
\alias{h5_write_dataset}
\title{Write a dataset to an HDF5 group}
\usage{
h5_write_dataset(
  h5_group,
  path,
  data,
  chunk_dims = NULL,
  compression_level = 0,
  dtype = NULL
)
}
\arguments{
\item{h5_group}{An `H5Group` object used as the starting location for `path`.}

\item{path}{Character string giving the dataset path relative to `h5_group`.}

\item{data}{Numeric matrix/array to write.}

\item{chunk_dims}{Optional integer vector specifying HDF5 chunk dimensions.}

\item{compression_level}{Integer 0â€“9 giving gzip compression level.}

\item{dtype}{Optional data type specification for HDF5 storage.}
}
\value{
Invisibly returns `TRUE` on success.
}
\description{
Creates or overwrites a dataset at `path`, optionally using
  chunking and gzip compression. Intermediate groups in `path` are created as
  needed. If `chunk_dims` is `NULL`, a heuristic attempts to keep chunks
  around 1 MiB. For datasets larger than 4 GiB, the fastest changing axis is
  halved until the estimated chunk size is below 1 GiB. If the resulting chunk
  would still exceed roughly 256 MiB (HDF5 practical limit), an additional
  reduction is performed with a warning.
}
